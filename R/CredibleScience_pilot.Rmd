---
title: "Credibility of science"
output: html_document
date: '2024-5-8'
---


# Packages

```{r, warning=FALSE, echo=TRUE}
set.seed(2024)
suppressPackageStartupMessages({
  library(dplyr)
  library(readr)
  library(tidyr) 
  library(ggplot2)
  library(metafor)
  library(here)
  library(zcurve)
  })
```

# Function

Load custom functions used to compute replicability, prior probability, statistical power, magnitude error, and sign error.

```{r}

# functions for the mixture distribution
dmix = function(x,p,m,s){
  drop(p %*% sapply(x, function(x) dnorm(x,mean=m,sd=s)))
}

rmix = function(n,p,m,s){    # sample from a normal mixture
  d=rmultinom(n,1,p)
  rnorm(n,m%*%d,s%*%d)
}

pmix = function(x,p,m,s){ # cumulative distr (vector x)
  drop(p %*% sapply(x, function(x) pnorm(x,mean=m,sd=s)))
}

# Estimate k-component zero-mean normal mixture distribution
mix = function(z,k=3,c1=0,c2=10^6,weights=1){
  # log likelihood function
  loglik = function(theta,z,k,weights=1){
    p=c(theta[1:(k-1)],1-sum(theta[1:(k-1)]))
    s=theta[k:(2*k-1)]
    m=rep(0,k)
    lik1=(abs(z) < c1)*(pmix(c1,p,m=m,s=s) - pmix(-c1,p,m=m,s=s))
    lik2=(abs(z) >= c1)*(abs(z) < c2)*dmix(z,p,m=m,s=s)
    lik3=(abs(z) >= c2)*(pmix(-c2,p,m=m,s=s) + 1 - pmix(c2,p,m=m,s=s))
    lik=lik1+lik2+lik3
    return(-sum(weights*log(lik)))   
  }

  ui=c(rep(-1,(k-1)),rep(0,k))         # (k-1) mixture props sum to < 1
  ui=rbind(ui,cbind(diag(2*k-1)))
  ci=c(-1,rep(0,k-1),rep(1,k))
  
  theta0=c(rep(1/k,(k-1)),c(1.2,2:k))
  opt=constrOptim(theta=theta0,f=loglik,ui=ui,ci=ci,
                  method = "Nelder-Mead",
                  z=z,weights=weights,k=k,
                  control=list(maxit=10^4))
  
  p=c(opt$par[1:(k-1)],1-sum(opt$par[1:(k-1)]))  
  sigma=opt$par[k:(2*k-1)]                       
  m=rep(0,k)                                     
  df=data.frame(p=p,m=m,sigma=sigma)
  return(df)
}

# posterior distribution
posterior <- function(z,p,m,s) { 
  s2=s^2
  p=p*dnorm(z,m,sqrt(s2+1))
  p <- p/sum(p)        
  pm <- z*s2/(s2+1) + m/(s2+1) 
  pv <- s2/(s2+1)         
  ps <- sqrt(pv)           
  data.frame(p,pm,ps)
}

# probability of replication
replcalc <- function(z,p,m,s,multiplier=1){
  post=posterior(abs(z),p=p,m=m,s=s)
  pp=post$p
  pm=sqrt(multiplier)*post$pm
  ps=sqrt(multiplier)*post$ps
  1 - pmix(1.96,p=pp,m=pm,s=sqrt(ps^2 + 1))
}

# compute pre-study (prior) probability
calcRodds <- function(FDR, pwr, alpha = 0.05) {
  R = alpha * (1 - FDR) / (pwr * FDR) # prior odds
  Pr = R / (R + 1) # prior probability
  return(Pr)
}


# power (two-tail) for meta-analysis
powerMA <- function(mu, SE, alpha = 0.05) {
  1 - pnorm(qnorm(1 - 0.05 / 2) - abs(mu) / SE) + pnorm(-qnorm(1 - 0.05 / 2)-abs(mu) / SE)
} 

# S error for meta-analysis
errorS <- function(mu, se, alpha = 0.05){
  p.u <- 1 - pnorm(qnorm(1 - alpha/2) - abs(mu)/se) 
  p.l <- pnorm(-qnorm(1 - alpha/2) - abs(mu)/se) 
  power <- p.u + p.l 
  errorS <- p.l/power 
  return(errorS)
} 

errorM <- function(mu, se, alpha = 0.05, N = 1e5) {
    est.random <- rnorm(n=N, mean = mu, sd = se)
    sig.index <- suppressWarnings(abs(est.random) > se*qnorm(1 - alpha/2))
    overestimate <- mean(abs(est.random)[sig.index])/abs(mu) 
    absolute_error <- overestimate*abs(mu) - abs(mu)
    relative_error <- absolute_error/(overestimate*abs(mu))
  return(abs(overestimate) %>% round(3))
}
```


# General introduction

As said in previous email, we want to estimate the trustworthiness/credibility/reliability of science.

In this paper, we focus on the evidence at meta-analytic level. The reasons are:

1. Results from the meta-analysis are considered as the “best evidence” and “gold standard”.

2. We can get the publication-bias corrected meta-analytic results. Across discipline publication-bias corrected meta-analytic results can be a good proxy of scientific evidence.

3. The trustworthiness of science at meta-analytic level has rarely been done. Most current studies focus on the primary study level.

Assessing the trustworthiness of science is a multifaceted endeavour, with no single indicator providing a sufficient assessment. Nonetheless, we can use some key statistical properties of the published evidence to reflect its credibility.

# Data

Load the meta-analytic level point estimates and uncertainty provided by František.

```{r}
# model estimates
dat <- read.csv("est.csv")
# rename key variables
names(dat)[3:7] <- c("mu_adj", "se_adj", "mu_unadj", "se_unadj", "discipline")
nrow(dat)
```

Amazing! We have 68524 estimates.

Next, we calculate statistical evidence (or, equivalently, z score in the NHST framework).
```{r}
dat <- dat %>% mutate(z_adj = mu_adj / se_adj, # after adjusting publication bias
                      z_unadj = mu_unadj / se_unadj) # before adjusting publication bias
```

# Overestimation of statistical evidence

The first index we want to calculate is the inflation of statistical evidence. 

Overall: 
```{r}
# overall
mean( (dat$z_unadj / mean(dat$z_adj)) ) # 1.877645
```

Economics:
```{r}
# economics
mean( (filter(dat, discipline=="economics")$z_unadj / mean(filter(dat, discipline=="economics")$z_adj)) ) # 4.126645
```

Environment:
```{r}
# environment
mean( (filter(dat, discipline=="environmental")$z_unadj / mean(filter(dat, discipline=="environmental")$z_adj)) )# 1.966678
```

Psychology:
```{r}
# psychology
mean( (filter(dat, discipline=="psychology")$z_unadj / mean(filter(dat, discipline=="psychology")$z_adj)) ) # 1.94695
```

Medicine:
```{r}
# medicine
mean( (filter(dat, discipline=="medicine")$z_unadj / mean(filter(dat, discipline=="medicine")$z_adj)) ) # 1.857167
```

We see that the statistical evidence (z score) derived from more than 68,000 meta-analyses was inflated by approximately 2-fold due to publication bias, raising concerns regarding the validity of null-hypothesis significance tests in the published meta-analyses.

# Replicability

Next index we want to calculate is the replicability of the publication-bias-adjusted-point-estimate of meta-analysis (meta-analytic mean effect after adjusting publication bias). 

I will use two methods. The first will be __van Zwet's method__. The second is __Frantisek's method__ which relaxes the assumption of van Zwet's method.

## Replicability based on van Zwet's method 

Briefly, we define the signal-noise-ratio ($SNR$) as the strength of true meta-analytic mean effect (the signal) relative standard error of the effect size estimate:
$$SNR = \mu / SE[\overline{\mu}]$$
Then, we use a mixture model to estimate the marginal density of the ($SNR$) across more than 68,000 meta-analyse, assuming each component of distribution has a zero-mean. We use a custom function. To save time, we use the subset of economic as an example. 

The weight (proportion), mean (which is assumed to be zero), and standard deviation of the mixture component can be obtained by:

```{r}
# economics
## subset economic data
dat_eco <- dat %>% filter(discipline == "economics")
## fit mixture model assuming zero mean
mix.df <- mix(z=dat_eco$z_adj,k=4,c1=0,c2=10,weights=1)
## extract weight (proportion)
p <- mix.df$p
## extract mean
m <- mix.df$m
## extract standard deviation
sigma <- mix.df$sigma
## show the whole data frame
data.frame(round(mix.df,3))
```

Next, we use a simple Monte Carlo simulation to generate a size of 1e7 exact replications of meta-analytic mean effect sizes based on the estimated marginal density of the ($SNR$):

```{r}
snr=rmix(10^7,p=p,m=m,s=sqrt(sigma^2-1)) # sqrt(sigma^2-1) is the standard deviation of SNR
```

The probability of a “successful replication”, replicability, is the probability that an exact replication meta-analytic will obtain a two-sided P value less than 0.05 with the estimate in the same direction as the original meta-analysis. 

With this collection of 1e7 exact replications, we can compute the applicability at the typical p-values, re-formulated as the scales of evidence strength according to according to Bland M. 2015. An Introduction to Medical Statistics. Oxford, UK: Oxford Univ. Press. 4th ed.

Our purpose is to build up the intuitive relationship between the replicability and evidence strength, albeit interpreting p-values (we could could consider BFs) as evidence strength is controversial.

```{r, echo=TRUE, warning=FALSE}
# typical p
pval=c(0.1,0.05,0.01,0.001,0.0001)
zval=qnorm(1 - pval/2)
strength = c("No evidence", "Weak evidence","Moderate evidence",
             "Strong evidence","Very strong evidence")
strength = factor(strength,
                  levels=c("No evidence", "Weak evidence",
                           "Moderate evidence","Strong evidence",
                           "Very strong evidence"))

replicate <- sapply(zval,replcalc,p=p,m=m,s=sqrt(sigma^2-1))
zval <- round(zval,2)
replicate <- round(replicate,2)
data.frame(strength,pval,zval,replicate)
```

We see that p-value just below 0.05 is only corresponding to a replicability around 35%. So, a p-value < 0.05 is not never a guarantee of replicability.


## Replicability based on Frantisek's method 

I am not sure whether I did it correctly in using Frantisek's method. Anyway, Frantisek will check it and show me the correct way if I am wrong.

First, let's run Frantisek's code (from a `.R` named `demonstration`) using the subset of economic.

```{r}
library(flexmix)
library(DEoptim)

### custom functions for fitting mixture models of absolute z-statistics
# these functions are very similar to z-curve
# the main difference is that these models:
# - use all test statistics (while z-curve uses only significant results by default)
# - estimate location and standard deviations of the mixture (while z-curve fixes means to 0:7 and standard deviations to 1)
zdist_pdf   <- function(x, mean, sd){
  l1 <- dnorm(x,  mean, sd)
  l2 <- dnorm(x, -mean, sd)
  return(l1 + l2)
}
fit_wzdist  <- function(x, w, fix_sd = FALSE, min_sd = FALSE){

  if(isFALSE(fix_sd)){
    fit <- DEoptim(
      fn  = function(par, x, w){
        -sum(w*log(zdist_pdf(x, mean = par[1], sd = par[2])))
      },
      x = x,
      w = w,
      lower   = c(0,   if(isFALSE(min_sd)) 0.001 else min_sd),
      upper   = c(100, 100),
      control = list(
        trace = FALSE
      )
    )
    param <- as.list(fit$optim$bestmem)
    names(param) <- c("mean", "sd")
  }else{
    fit <- DEoptim(
      fn  = function(par, x, w, fix_sd){
        -sum(w*log(zdist_pdf(x, mean = par[1], sd = fix_sd)))
      },
      x      = x,
      fix_sd = fix_sd,
      w      = w,
      lower   = c(0),
      upper   = c(100),
      control = list(
        trace = FALSE
      )
    )
    param <- as.list(fit$optim$bestmem)
    names(param) <- c("mean")
  }

  return(param)
}
zdist_clust <- function(formula = .~., fix_sd = FALSE, min_sd = FALSE){

  retval <- new("FLXMC", weighted = TRUE,
                formula = formula, dist = "zdist",
                name = "folded normal distributions")

  retval@defineComponent <- function(param) {

    logLik <- function(x, y) {
      log(zdist_pdf(y, mean = param$mean, sd = param$sd))
    }

    predict <- function(x) {
      matrix(param$mean, nrow = nrow(x), ncol = length(param$mean), byrow = TRUE)
    }

    new("FLXcomponent",
        parameters = param,
        df         = param$df,
        logLik     = logLik,
        predict    = predict)
  }

  retval@fit <- function(x, y, w, ...) {
    param <- fit_wzdist(y, w, fix_sd, min_sd)
    if(!isFALSE(fix_sd)){
      param$sd <- fix_sd
    }
    retval@defineComponent(c(param, df = if(isFALSE(fix_sd)) 2 else 1))
  }
  return(retval)
}
density_zdist_clust <- function(x, fit){
  weights <- fit@size / sum(fit@size)
  param   <- parameters(fit)
  x_dens  <- do.call(rbind, lapply(seq_along(weights), function(i)
    weights[i] * zdist_pdf(x, param["mean",i], param["sd",i])
  ))
  return(colSums(x_dens))
}
density_snr_clust   <- function(x, fit, ...){

  weights <- fit@size / sum(fit@size)
  param   <- parameters(fit)

  # add continuous density
  x_dens  <- do.call(rbind, lapply(seq_along(weights), function(i)
    if(param["sd",i] > 1){
      weights[i] * zdist_pdf(x, param["mean",i], sqrt(param["sd",i]^2 - 1))
    }else{
      rep(0, length(x))
    }
  ))
  if(any(colSums(x_dens) > 0)){
    lines(x, colSums(x_dens), ...)
  }

  # add fixed density
  for(i in seq_along(weights)){
    if(param["sd",i] == 1){
      abline(v = param["mean",i], ..., lwd = 10*weights[i])
    }
  }
  return(invisible(NA))
}
```

Next, we fit mixture models that allow for non-centered signal-to-noise. 
```{r}
# fixes the standard deviation to 1 (i.e., a snr-curve model with variable means)
fit_snr <- flexmix(dat_eco$z_adj ~ 1, k = 4, model = zdist_clust(min_sd = 1))
```

Check the estimated parameters of the mixture model:

```{r}
summary(fit_snr)
parameters(fit_snr)
```

I did not know what exactly happen but I specify 4 components of normal distribution, but flexmix only returned me 2.

Now with the estimated weight (proportion), mean (which is assumed to be zero), and standard deviation of the mixture in hand, we can calculate the replicability as we did in van Zwet's method.

```{r}
# extract mixture proportions
p=summary(fit_snr)@comptab$prior  
# extract # means
m=c(parameters(fit_snr)[1,1], parameters(fit_snr)[1,2]) 
# extract sigmas 
sigma=c(parameters(fit_snr)[2,1], parameters(fit_snr)[2,2]) # sigmas 
```

Then, we write a function for computing replication probability using the non-symmetric (non-zero mean) distribution of the SNR. One of van Zwet's paper provides the formula and R code.

```{r}
# function for the distribution of the SNR is not symmetric (non-zero mean)
replcalc2 <- function(z_orig){ # compute replication probability (predictive power) when original study produced z
  z=abs(z_orig)
  pr=dmix(z,p,m,sigma) / (dmix(z,p,m,sigma) + dmix(-z,p,m,sigma)) # pr(z >0 | |z|)
  pr=drop(pr)
  tau=sigma # I assume sigma estimated from fit_snr is the sd of SNR; otherwise, we need to replace it as sqrt(sigma^2-1).
  post=posterior(z,p,m,tau) # p(SNR|z= |z|)
  pm=post$pm
  ps=post$ps
  powpos=1 - pmix(1.96,p=post$p,m=pm,s=sqrt(ps^2 + 1)) # signif given z=|z|
  post=posterior(-z,p,m,tau) # p(SNR|z=-|z|)
  pm=post$pm
  ps=post$ps
  powneg= pmix(-1.96,p=post$p,m=pm,s=sqrt(ps^2 + 1))   # signif given z=-|z|
  return(as.numeric(pr*powpos + (1-pr)*powneg)) # signif given |z|
}
```

Now, we can build up the intuitive relationship between the replicability and evidence strength, based on Frantksek's distribution of SNR

```{r, echo=TRUE, warning=FALSE}
# typical p
pval=c(0.1,0.05,0.01,0.001,0.0001)
zval=qnorm(1 - pval/2)
strength = c("No evidence", "Weak evidence","Moderate evidence",
             "Strong evidence","Very strong evidence")
strength = factor(strength,
                  levels=c("No evidence", "Weak evidence",
                           "Moderate evidence","Strong evidence",
                           "Very strong evidence"))


# calculate replicability
replicate <- sapply(zval, function(z_orig) {
  replcalc2(z_orig)
})

zval <- round(zval,2)
replicate2 <- round(replicate,2)
data.frame(strength,pval,zval,replicate2)
```


# Power, magnitude and sign error

The third index is a set a statistical errors like Type II error (1 - power), magnitude error, and sign error.

These statistical errors are easy to get given a z-score. For example, power is just a transformation of z-score (or p-value).

Power:

```{r}
# power
dat_eco <- dat_eco %>% 
  mutate(pwr_unadj = powerMA(mu = mu_unadj, SE = se_unadj), # power to detect unadjusted mean effect 
         pwr_adj = powerMA(mu = mu_adj, SE = se_adj)) # power to detect adjusted mean effect
# summary
summary(dat_eco$pwr_adj)
```

Magnitude error:

```{r}
# magnitude error
M_unadj <- numeric(nrow(dat_eco))
for (i in 1:nrow(dat_eco)) {
  M_unadj[i] <- errorM(mu = dat_eco$mu_unadj[i], se = dat_eco$se_unadj[i])
} # error in detecting unadjusted mean effect
dat_eco$M_unadj <- M_unadj

M_adj <- numeric(nrow(dat_eco))
for (i in 1:nrow(dat_eco)) {
  M_adj[i] <- errorM(mu = dat_eco$mu_adj[i], se = dat_eco$se_adj[i])
} # error in detecting adjusted mean effect
dat_eco$M_adj <- M_adj
# summary
summary(dat_eco$M_adj)
```

Sign error:
```{r}
# S
S_unadj <- numeric(nrow(dat_eco))
for (i in 1:nrow(dat_eco)) {
  S_unadj[i] <- errorS(mu = dat_eco$mu_unadj[i], se = dat_eco$se_unadj[i])
} # sign error in detecting unadjusted mean effect
dat_eco$S_unadj <- S_unadj

S_adj <- numeric(nrow(dat_eco))
for (i in 1:nrow(dat_eco)) {
  S_adj[i] <- errorS(mu = dat_eco$mu_adj[i], se = dat_eco$se_adj[i])
} # sign error in detecting adjusted mean effect
dat_eco$S_adj <- S_adj
# summary
summary(dat_eco$S_adj)
```


# Prior probability

The fouth index is prior probability of a hypothesis being true before testing. I am not confident with the way I used to compute this index. So, it is great to have to check it.

My strategy is first to use `z-curve` package (`swfdr` is also possible) to estimate the false discovery rate or false positive report probability (1 - positive predictive value), and then the formula to convert it into the prior probability. The formula can be found elsewhere, for example, Button K S, Ioannidis J P A, Mokrysz C, et al. Power failure: why small sample size undermines the reliability of neuroscience[J]. Nature reviews neuroscience, 2013, 14(5): 365-376.

First, we look at the estimates from `zcurve`:
```{r}
# fit a z curve
res <- zcurve(z = dat_eco$z_adj, method = "EM", bootstrap = F) # to save time, I did not compute CI
res.est <- summary(res, all = TRUE)
res.est
```

`Soric FDR = 0.075` is the value we want to use for the following computation of prior probability.

```{r}
# write a function to calculate prior probability
calcRodds <- function(FDR, pwr, alpha = 0.05) {
  R = alpha * (1 - FDR) / (pwr * FDR)
  Pr = R / (R + 1)
  return(Pr)
}

# to calculate priority probability, we need FDR, power and alpha.
# now, we have PDR, and power, and  we can assume alpha = 0.05. 
Pr_eco <- calcRodds(FDR = res.est$coefficients[3], pwr = dat_eco$pwr_adj, alpha = 0.05) 

# summary
summary(Pr_eco)
```
