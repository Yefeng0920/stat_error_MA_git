---
title: "Trustworthiness of science"
output: html_document
date: '2024-03-18'
---


# Packages

```{r, warning=FALSE, echo=TRUE}
set.seed(2024)
suppressPackageStartupMessages({
  library(dplyr)
  library(readr)
  library(tidyr) 
  library(ggplot2)
  library(metafor)
  library(here)
  library(zcurve)
  })
```

# Function

Load custom functions used to compute replicability, prior probability, statistical power, magnitude error, and sign error.

```{r}

# functions for the mixture distribution
dmix = function(x,p,m,s){
  drop(p %*% sapply(x, function(x) dnorm(x,mean=m,sd=s)))
}

rmix = function(n,p,m,s){    # sample from a normal mixture
  d=rmultinom(n,1,p)
  rnorm(n,m%*%d,s%*%d)
}

pmix = function(x,p,m,s){ # cumulative distr (vector x)
  drop(p %*% sapply(x, function(x) pnorm(x,mean=m,sd=s)))
}

# Estimate k-component zero-mean normal mixture distribution
mix = function(z,k=3,c1=0,c2=10^6,weights=1){
  # log likelihood function
  loglik = function(theta,z,k,weights=1){
    p=c(theta[1:(k-1)],1-sum(theta[1:(k-1)]))
    s=theta[k:(2*k-1)]
    m=rep(0,k)
    lik1=(abs(z) < c1)*(pmix(c1,p,m=m,s=s) - pmix(-c1,p,m=m,s=s))
    lik2=(abs(z) >= c1)*(abs(z) < c2)*dmix(z,p,m=m,s=s)
    lik3=(abs(z) >= c2)*(pmix(-c2,p,m=m,s=s) + 1 - pmix(c2,p,m=m,s=s))
    lik=lik1+lik2+lik3
    return(-sum(weights*log(lik)))   
  }

  ui=c(rep(-1,(k-1)),rep(0,k))         # (k-1) mixture props sum to < 1
  ui=rbind(ui,cbind(diag(2*k-1)))
  ci=c(-1,rep(0,k-1),rep(1,k))
  
  theta0=c(rep(1/k,(k-1)),c(1.2,2:k))
  opt=constrOptim(theta=theta0,f=loglik,ui=ui,ci=ci,
                  method = "Nelder-Mead",
                  z=z,weights=weights,k=k,
                  control=list(maxit=10^4))
  
  p=c(opt$par[1:(k-1)],1-sum(opt$par[1:(k-1)]))  
  sigma=opt$par[k:(2*k-1)]                       
  m=rep(0,k)                                     
  df=data.frame(p=p,m=m,sigma=sigma)
  return(df)
}

# posterior distribution
posterior <- function(z,p,m,s) { 
  s2=s^2
  p=p*dnorm(z,m,sqrt(s2+1))
  p <- p/sum(p)        
  pm <- z*s2/(s2+1) + m/(s2+1) 
  pv <- s2/(s2+1)         
  ps <- sqrt(pv)           
  data.frame(p,pm,ps)
}

# probability of replication
replcalc <- function(z,p,m,s,multiplier=1){
  post=posterior(abs(z),p=p,m=m,s=s)
  pp=post$p
  pm=sqrt(multiplier)*post$pm
  ps=sqrt(multiplier)*post$ps
  1 - pmix(1.96,p=pp,m=pm,s=sqrt(ps^2 + 1))
}

# compute pre-study (prior) probability
calcRodds <- function(FDR, pwr, alpha = 0.05) {
  R = alpha * (1 - FDR) / (pwr * FDR) # prior odds
  Pr = R / (R + 1) # prior probability
  return(Pr)
}


# power (two-tail) for meta-analysis
powerMA <- function(mu, SE, alpha = 0.05) {
  1 - pnorm(qnorm(1 - 0.05 / 2) - abs(mu) / SE) + pnorm(-qnorm(1 - 0.05 / 2)-abs(mu) / SE)
} 

# S error for meta-analysis
errorS <- function(mu, se, alpha = 0.05){
  p.u <- 1 - pnorm(qnorm(1 - alpha/2) - abs(mu)/se) 
  p.l <- pnorm(-qnorm(1 - alpha/2) - abs(mu)/se) 
  power <- p.u + p.l 
  errorS <- p.l/power 
  return(errorS)
} 

errorM <- function(mu, se, alpha = 0.05, N = 1e5) {
    est.random <- rnorm(n=N, mean = mu, sd = se)
    sig.index <- suppressWarnings(abs(est.random) > se*qnorm(1 - alpha/2))
    overestimate <- mean(abs(est.random)[sig.index])/abs(mu) 
    absolute_error <- overestimate*abs(mu) - abs(mu)
    relative_error <- absolute_error/(overestimate*abs(mu))
  return(abs(overestimate) %>% round(3))
}
```


# General introduction

As said in previous email, we want to estimate the trustworthiness/credibility/reliability of science.

In this paper, we focus on the evidence at meta-analytic level. The reasons are:

1. Results from the meta-analysis are considered as the “best evidence” and “gold standard”.

2. We can get the publication-bias corrected meta-analytic results. Across discipline publication-bias corrected meta-analytic results can be a good proxy of scientific evidence.

3. The trustworthiness of science at meta-analytic level has rarely been done. Most current studies focus on the primary study level.

Assessing the trustworthiness of science is a multifaceted endeavour, with no single indicator providing a sufficient assessment. Nonetheless, we can use some key statistical properties of the published evidence to reflect its credibility.

# Data

Load the meta-analytic level point estimates and uncertainty provided by František.

```{r}
# model estimates
dat <- read.csv("merged.csv")
# rename key variables
names(dat)[3:7] <- c("mu_adj", "se_adj", "mu_unadj", "se_unadj", "discipline")
nrow(dat)
```

Amazing! We have 68524 estimates.

Next, we calculate statistical evidence (or, equivalently, z score in the NHST framework).
```{r}
dat <- dat %>% mutate(z_adj = mu_adj / se_adj, # after adjusting pb
                      z_unadj = mu_unadj / se_unadj) # before adjusting pb
```

# Overestimation of statistical evidence

The first index we want to calculate is the inflation of statistical evidence. 

Overall: 
```{r}
# overall
mean( (dat$z_unadj / mean(dat$z_adj)) ) # 1.877645
```

Economics:
```{r}
# economics
mean( (filter(dat, discipline=="economics")$z_unadj / mean(filter(dat, discipline=="economics")$z_adj)) ) # 4.126645
```

Environment:
```{r}
# environment
mean( (filter(dat, discipline=="environmental")$z_unadj / mean(filter(dat, discipline=="environmental")$z_adj)) )# 1.966678
```

Psychology:
```{r}
# psychology
mean( (filter(dat, discipline=="psychology")$z_unadj / mean(filter(dat, discipline=="psychology")$z_adj)) ) # 1.94695
```

Medicine:
```{r}
# medicine
mean( (filter(dat, discipline=="medicine")$z_unadj / mean(filter(dat, discipline=="medicine")$z_adj)) ) # 1.857167
```

We see that the statistical evidence (z score) derived from more than 68,000 meta-analyses was inflated by approximately 2-fold due to publication bias, raising concerns regarding the validity of null-hypothesis significance tests in the published meta-analyses.

# Replicability

Next index we want to calculate is the replicability of the publication-bias-adjusted-point-estimate of meta-analysis (meta-analytic mean effect after adjusting publication bias). This is a bit complex, but I am quite confident with the computation.

Briefly, we define the signal-noise-ratio ($SNR$) as the strength of true eta-analytic mean effect (the signal) relative standard error of the effect size estimate:
$$SNR = \mu / SE[\overline{\mu}]$$
Then, we use a mixture model to estimate the marginal density of the ($SNR$) across more than 68,000 meta-analyses. We use a custom function. To save time, we use the subset of economic as an example. 

The weight, mean, and standard deviation of the mixture component can be obtained by:

```{r}
# economics
dat_eco <- dat %>% filter(discipline == "economics")
mix.df <- mix(z=dat_eco$z_adj,k=4,c1=0,c2=10,weights=1)
p <- mix.df$p
m <- mix.df$m
sigma <- mix.df$sigma
```

Next, we use a simple Monte Carlo simulation to generate a size of 1e7 exact replications of meta-analytic mean effect sizes based on the estimated estimate the marginal density of the ($SNR$):

```{r}
snr=rmix(10^7,p=p,m=m,s=sqrt(sigma^2-1))
```

The probability of a “successful replication”, replicability, is the the probability that an exact replication meta-analytic will obtain a two-sided P value less than 0.05 with the estimate in the same direction as the original meta-analysis. 

With this collection of 1e7 exact replications, we can compute the applicability at the typical p-values, re-formulated as the scales of evidence strength according to according to Bland M. 2015. An Introduction to Medical Statistics. Oxford, UK: Oxford Univ. Press. 4th ed.

Our purpose is to build up the intuitive relationship between the replicability and evidence strength, albeit interpreting p-values (we could could consider BFs) as evidence strength is controversial.

```{r, echo=TRUE, warning=FALSE}
# typical p
pval=c(0.1,0.05,0.01,0.001,0.0001)
zval=qnorm(1 - pval/2)
strength = c("No evidence", "Weak evidence","Moderate evidence",
             "Strong evidence","Very strong evidence")
strength = factor(strength,
                  levels=c("No evidence", "Weak evidence",
                           "Moderate evidence","Strong evidence",
                           "Very strong evidence"))

replicate <- sapply(zval,replcalc,p=p,m=m,s=sqrt(sigma^2-1))
zval <- round(zval,2)
replicate <- round(replicate,2)
data.frame(strength,pval,zval,replicate)
```

We see that p-value just below 0.05 is only corresponding to a replicability around 35%. So, a p-value < 0.05 is not never a guarantee of replicability.


# Power, magnitude and sign error

The third index is a set a statistical errors like Type II error (1 - power), magnitude error, and sign error.

These statistical errors are easy to get given a z-score. For example, power is just a transformation of z-score (or p-value).

Power:

```{r}
# power
dat_eco <- dat_eco %>% 
  mutate(pwr_unadj = powerMA(mu = mu_unadj, SE = se_unadj), # power to detect unadjusted mean effect 
         pwr_adj = powerMA(mu = mu_adj, SE = se_adj)) # power to detect adjusted mean effect
```

Magnitude error:

```{r}
# magnitude error
M_unadj <- numeric(nrow(dat_eco))
for (i in 1:nrow(dat_eco)) {
  M_unadj[i] <- errorM(mu = dat_eco$mu_unadj[i], se = dat_eco$se_unadj[i])
} # error in detecting unadjusted mean effect
dat_eco$M_unadj <- M_unadj

M_adj <- numeric(nrow(dat_eco))
for (i in 1:nrow(dat_eco)) {
  M_adj[i] <- errorM(mu = dat_eco$mu_adj[i], se = dat_eco$se_adj[i])
} # error in detecting adjusted mean effect
dat_eco$M_adj <- M_adj
```

Sign error:
```{r}
# S
S_unadj <- numeric(nrow(dat_eco))
for (i in 1:nrow(dat_eco)) {
  S_unadj[i] <- errorS(mu = dat_eco$mu_unadj[i], se = dat_eco$se_unadj[i])
} # sign error in detecting unadjusted mean effect
dat_eco$S_unadj <- S_unadj

S_adj <- numeric(nrow(dat_eco))
for (i in 1:nrow(dat_eco)) {
  S_adj[i] <- errorS(mu = dat_eco$mu_adj[i], se = dat_eco$se_adj[i])
} # sign error in detecting adjusted mean effect
dat_eco$S_adj <- S_adj
```


# Prior probability

The fourth index is prior probability of a hypothesis being true before testing. I am not confident with the way I used to compute this index. So, it is great to have to check it.

My strategy is first to use `z-curve` package (`swfdr` is also possible) to estimate the false discovery rate or false positive report probability (1 - positive predictive value), and then the formula to convert it into the prior probability. The formula can be found elsewhere, for example, Button K S, Ioannidis J P A, Mokrysz C, et al. Power failure: why small sample size undermines the reliability of neuroscience[J]. Nature reviews neuroscience, 2013, 14(5): 365-376.

```{r}
# fit a z curve
res <- zcurve(z = dat_eco$z_adj, method = "EM", bootstrap = F) # to save time, I did not compute CI
res.est <- summary(res, all = TRUE)

# write a function to calculate prior probability
calcRodds <- function(FDR, pwr, alpha = 0.05) {
  R = alpha * (1 - FDR) / (pwr * FDR)
  Pr = R / (R + 1)
  return(Pr)
}

# to calculate priority probability, we need FDR, power and alpha.
# now, we have PDR, and power, and  we can assume alpha = 0.05. 

calcRodds(FDR = res.est$coefficients[3], pwr = , alpha = 0.05) # 0.545449


```
